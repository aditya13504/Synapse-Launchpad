{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partner Recommender Training Pipeline\n",
    "\n",
    "This notebook demonstrates how to train the HugeCTR two-tower model for startup partner matching.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The model uses a two-tower architecture:\n",
    "- **Company A Tower**: Processes features for the query company\n",
    "- **Company B Tower**: Processes features for candidate partners\n",
    "- **Similarity Calculation**: Dot product + sigmoid for match probability\n",
    "\n",
    "## Features\n",
    "- Dense: user_overlap_score, funding_amount, employee_count, growth_rate, market_sentiment\n",
    "- Sparse: company_id, industry, stage, technologies (categorical embeddings)\n",
    "- Culture Vector: 128-dimensional embedding from NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import hugectr\n",
    "from hugectr import Session, solver_parser_helper, get_learning_rate_scheduler\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.core.utils import Distributed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = \"/app\"\n",
    "DATA_DIR = f\"{BASE_DIR}/data\"\n",
    "MODEL_DIR = f\"{BASE_DIR}/models\"\n",
    "TRAINING_DIR = f\"{DATA_DIR}/training\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(TRAINING_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training started at: {datetime.now()}\")\n",
    "print(f\"CUDA devices available: {os.getenv('CUDA_VISIBLE_DEVICES', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Generate synthetic training data for partner matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=100000):\n",
    "    \"\"\"\n",
    "    Generate synthetic training data for partner matching\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Company IDs\n",
    "    num_companies = 1000\n",
    "    company_ids = [f\"company_{i}\" for i in range(num_companies)]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Select two different companies\n",
    "        company_a, company_b = np.random.choice(company_ids, 2, replace=False)\n",
    "        \n",
    "        # Generate features for company A\n",
    "        funding_a = np.random.lognormal(15, 2)  # Log-normal distribution for funding\n",
    "        employees_a = np.random.randint(10, 1000)\n",
    "        growth_a = np.random.normal(25, 15)  # Growth rate percentage\n",
    "        sentiment_a = np.random.normal(0, 0.3)  # Market sentiment\n",
    "        overlap_score = np.random.beta(2, 5)  # User overlap score\n",
    "        \n",
    "        # Generate features for company B (with some correlation to A)\n",
    "        funding_b = funding_a * np.random.lognormal(0, 0.5)\n",
    "        employees_b = employees_a * np.random.lognormal(0, 0.3)\n",
    "        growth_b = growth_a + np.random.normal(0, 10)\n",
    "        sentiment_b = sentiment_a + np.random.normal(0, 0.2)\n",
    "        \n",
    "        # Generate culture vectors (128-dim)\n",
    "        culture_a = np.random.normal(0, 1, 128)\n",
    "        culture_b = culture_a + np.random.normal(0, 0.5, 128)  # Correlated\n",
    "        \n",
    "        # Calculate similarity features\n",
    "        funding_similarity = 1 - abs(np.log(funding_a) - np.log(funding_b)) / 10\n",
    "        size_similarity = 1 - abs(np.log(employees_a) - np.log(employees_b)) / 5\n",
    "        growth_similarity = 1 - abs(growth_a - growth_b) / 50\n",
    "        sentiment_similarity = 1 - abs(sentiment_a - sentiment_b)\n",
    "        culture_similarity = np.dot(culture_a, culture_b) / (np.linalg.norm(culture_a) * np.linalg.norm(culture_b))\n",
    "        \n",
    "        # Generate label based on similarity features\n",
    "        match_probability = (\n",
    "            funding_similarity * 0.2 +\n",
    "            size_similarity * 0.15 +\n",
    "            growth_similarity * 0.15 +\n",
    "            sentiment_similarity * 0.2 +\n",
    "            (culture_similarity + 1) / 2 * 0.3  # Normalize culture similarity\n",
    "        )\n",
    "        \n",
    "        # Add noise and threshold\n",
    "        match_probability += np.random.normal(0, 0.1)\n",
    "        match_probability = np.clip(match_probability, 0, 1)\n",
    "        \n",
    "        label = 1 if match_probability > 0.6 else 0\n",
    "        \n",
    "        # Create record\n",
    "        record = {\n",
    "            'company_a': company_a,\n",
    "            'company_b': company_b,\n",
    "            'funding_a': funding_a,\n",
    "            'employees_a': employees_a,\n",
    "            'growth_a': growth_a,\n",
    "            'sentiment_a': sentiment_a,\n",
    "            'funding_b': funding_b,\n",
    "            'employees_b': employees_b,\n",
    "            'growth_b': growth_b,\n",
    "            'sentiment_b': sentiment_b,\n",
    "            'overlap_score': overlap_score,\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "        # Add culture vectors\n",
    "        for i in range(128):\n",
    "            record[f'culture_a_{i}'] = culture_a[i]\n",
    "            record[f'culture_b_{i}'] = culture_b[i]\n",
    "        \n",
    "        data.append(record)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate training data\n",
    "print(\"Generating synthetic training data...\")\n",
    "df = generate_synthetic_data(100000)\n",
    "\n",
    "print(f\"Generated {len(df)} training samples\")\n",
    "print(f\"Positive samples: {df['label'].sum()} ({df['label'].mean():.2%})\")\n",
    "print(f\"Negative samples: {len(df) - df['label'].sum()} ({1 - df['label'].mean():.2%})\")\n",
    "\n",
    "# Display sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing with NVTabular\n",
    "\n",
    "Use NVTabular for GPU-accelerated data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to cuDF for GPU processing\n",
    "gdf = cudf.from_pandas(df)\n",
    "\n",
    "# Define feature columns\n",
    "categorical_cols = ['company_a', 'company_b']\n",
    "continuous_cols = [\n",
    "    'funding_a', 'employees_a', 'growth_a', 'sentiment_a',\n",
    "    'funding_b', 'employees_b', 'growth_b', 'sentiment_b',\n",
    "    'overlap_score'\n",
    "]\n",
    "\n",
    "# Culture vector columns\n",
    "culture_cols_a = [f'culture_a_{i}' for i in range(128)]\n",
    "culture_cols_b = [f'culture_b_{i}' for i in range(128)]\n",
    "culture_cols = culture_cols_a + culture_cols_b\n",
    "\n",
    "label_col = ['label']\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"Continuous features: {len(continuous_cols)}\")\n",
    "print(f\"Culture features: {len(culture_cols)}\")\n",
    "\n",
    "# Create NVTabular workflow\n",
    "categorical_features = categorical_cols >> Categorify()\n",
    "continuous_features = continuous_cols >> FillMissing() >> Normalize()\n",
    "culture_features = culture_cols >> FillMissing() >> Normalize()\n",
    "label_features = label_col >> LambdaOp(lambda x: x.astype('float32'))\n",
    "\n",
    "workflow_ops = categorical_features + continuous_features + culture_features + label_features\n",
    "workflow = nvt.Workflow(workflow_ops)\n",
    "\n",
    "# Create dataset\n",
    "dataset = nvt.Dataset(gdf)\n",
    "\n",
    "# Fit workflow\n",
    "print(\"Fitting NVTabular workflow...\")\n",
    "workflow.fit(dataset)\n",
    "\n",
    "# Transform data\n",
    "print(\"Transforming data...\")\n",
    "transformed_dataset = workflow.transform(dataset)\n",
    "transformed_gdf = transformed_dataset.to_ddf().compute()\n",
    "\n",
    "print(f\"Transformed data shape: {transformed_gdf.shape}\")\n",
    "print(\"Transformation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split and Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_size = int(0.8 * len(transformed_gdf))\n",
    "train_gdf = transformed_gdf.iloc[:train_size]\n",
    "val_gdf = transformed_gdf.iloc[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(train_gdf)}\")\n",
    "print(f\"Validation samples: {len(val_gdf)}\")\n",
    "\n",
    "# Save processed data\n",
    "train_path = f\"{TRAINING_DIR}/train_data.parquet\"\n",
    "val_path = f\"{TRAINING_DIR}/val_data.parquet\"\n",
    "\n",
    "train_gdf.to_parquet(train_path)\n",
    "val_gdf.to_parquet(val_path)\n",
    "\n",
    "print(f\"Training data saved to: {train_path}\")\n",
    "print(f\"Validation data saved to: {val_path}\")\n",
    "\n",
    "# Save workflow\n",
    "workflow_path = f\"{MODEL_DIR}/nvt_workflow\"\n",
    "workflow.save(workflow_path)\n",
    "print(f\"NVTabular workflow saved to: {workflow_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HugeCTR Model Configuration\n",
    "\n",
    "Define the two-tower model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hugectr_config():\n",
    "    \"\"\"\n",
    "    Create HugeCTR configuration for two-tower model\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"solver\": {\n",
    "            \"lr_policy\": \"fixed\",\n",
    "            \"display\": 1000,\n",
    "            \"max_iter\": 20000,\n",
    "            \"snapshot\": 10000,\n",
    "            \"snapshot_prefix\": f\"{MODEL_DIR}/partner_recommender\",\n",
    "            \"eval_interval\": 1000,\n",
    "            \"eval_batches\": 100,\n",
    "            \"mixed_precision\": 1024,\n",
    "            \"batchsize\": 1024,\n",
    "            \"batchsize_eval\": 1024,\n",
    "            \"lr\": 0.001,\n",
    "            \"warmup_steps\": 1000,\n",
    "            \"decay_start\": 15000,\n",
    "            \"decay_steps\": 5000,\n",
    "            \"decay_power\": 2.0,\n",
    "            \"end_lr\": 0.0\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"adam_hparam\": {\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"beta1\": 0.9,\n",
    "                \"beta2\": 0.999,\n",
    "                \"epsilon\": 0.0000001\n",
    "            }\n",
    "        },\n",
    "        \"layers\": [\n",
    "            # Data layer\n",
    "            {\n",
    "                \"name\": \"data\",\n",
    "                \"type\": \"Data\",\n",
    "                \"source\": train_path,\n",
    "                \"eval_source\": val_path,\n",
    "                \"check\": \"None\",\n",
    "                \"label\": {\n",
    "                    \"top\": \"label\",\n",
    "                    \"label_dim\": 1\n",
    "                },\n",
    "                \"dense\": {\n",
    "                    \"top\": \"dense\",\n",
    "                    \"dense_dim\": len(continuous_cols) + len(culture_cols)\n",
    "                },\n",
    "                \"sparse\": [\n",
    "                    {\n",
    "                        \"top\": \"company_a_cat\",\n",
    "                        \"type\": \"DistributedSlot\",\n",
    "                        \"max_feature_num_per_sample\": 1,\n",
    "                        \"slot_num\": 1\n",
    "                    },\n",
    "                    {\n",
    "                        \"top\": \"company_b_cat\",\n",
    "                        \"type\": \"DistributedSlot\",\n",
    "                        \"max_feature_num_per_sample\": 1,\n",
    "                        \"slot_num\": 1\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            # Sparse embeddings\n",
    "            {\n",
    "                \"name\": \"sparse_embedding_a\",\n",
    "                \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "                \"bottom\": \"company_a_cat\",\n",
    "                \"top\": \"sparse_embedding_a\",\n",
    "                \"sparse_embedding_hparam\": {\n",
    "                    \"vocabulary_size\": 10000,\n",
    "                    \"embedding_vec_size\": 128,\n",
    "                    \"combiner\": \"sum\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"sparse_embedding_b\",\n",
    "                \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "                \"bottom\": \"company_b_cat\",\n",
    "                \"top\": \"sparse_embedding_b\",\n",
    "                \"sparse_embedding_hparam\": {\n",
    "                    \"vocabulary_size\": 10000,\n",
    "                    \"embedding_vec_size\": 128,\n",
    "                    \"combiner\": \"sum\"\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # Company A tower\n",
    "            {\n",
    "                \"name\": \"reshape_a\",\n",
    "                \"type\": \"Reshape\",\n",
    "                \"bottom\": \"sparse_embedding_a\",\n",
    "                \"top\": \"reshape_a\",\n",
    "                \"leading_dim\": 128\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"concat_a\",\n",
    "                \"type\": \"Concat\",\n",
    "                \"bottom\": [\"dense\", \"reshape_a\"],\n",
    "                \"top\": \"concat_a\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"fc1_a\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"concat_a\",\n",
    "                \"top\": \"fc1_a\",\n",
    "                \"fc_param\": {\"num_output\": 512}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"relu1_a\",\n",
    "                \"type\": \"ReLU\",\n",
    "                \"bottom\": \"fc1_a\",\n",
    "                \"top\": \"relu1_a\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"dropout1_a\",\n",
    "                \"type\": \"Dropout\",\n",
    "                \"rate\": 0.5,\n",
    "                \"bottom\": \"relu1_a\",\n",
    "                \"top\": \"dropout1_a\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"fc2_a\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"dropout1_a\",\n",
    "                \"top\": \"fc2_a\",\n",
    "                \"fc_param\": {\"num_output\": 256}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"relu2_a\",\n",
    "                \"type\": \"ReLU\",\n",
    "                \"bottom\": \"fc2_a\",\n",
    "                \"top\": \"relu2_a\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"tower_a\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"relu2_a\",\n",
    "                \"top\": \"tower_a\",\n",
    "                \"fc_param\": {\"num_output\": 128}\n",
    "            },\n",
    "            \n",
    "            # Company B tower (similar structure)\n",
    "            {\n",
    "                \"name\": \"reshape_b\",\n",
    "                \"type\": \"Reshape\",\n",
    "                \"bottom\": \"sparse_embedding_b\",\n",
    "                \"top\": \"reshape_b\",\n",
    "                \"leading_dim\": 128\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"concat_b\",\n",
    "                \"type\": \"Concat\",\n",
    "                \"bottom\": [\"dense\", \"reshape_b\"],\n",
    "                \"top\": \"concat_b\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"fc1_b\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"concat_b\",\n",
    "                \"top\": \"fc1_b\",\n",
    "                \"fc_param\": {\"num_output\": 512}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"relu1_b\",\n",
    "                \"type\": \"ReLU\",\n",
    "                \"bottom\": \"fc1_b\",\n",
    "                \"top\": \"relu1_b\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"dropout1_b\",\n",
    "                \"type\": \"Dropout\",\n",
    "                \"rate\": 0.5,\n",
    "                \"bottom\": \"relu1_b\",\n",
    "                \"top\": \"dropout1_b\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"fc2_b\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"dropout1_b\",\n",
    "                \"top\": \"fc2_b\",\n",
    "                \"fc_param\": {\"num_output\": 256}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"relu2_b\",\n",
    "                \"type\": \"ReLU\",\n",
    "                \"bottom\": \"fc2_b\",\n",
    "                \"top\": \"relu2_b\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"tower_b\",\n",
    "                \"type\": \"InnerProduct\",\n",
    "                \"bottom\": \"relu2_b\",\n",
    "                \"top\": \"tower_b\",\n",
    "                \"fc_param\": {\"num_output\": 128}\n",
    "            },\n",
    "            \n",
    "            # Similarity calculation\n",
    "            {\n",
    "                \"name\": \"dot_product\",\n",
    "                \"type\": \"DotProduct\",\n",
    "                \"bottom\": [\"tower_a\", \"tower_b\"],\n",
    "                \"top\": \"dot_product\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"sigmoid\",\n",
    "                \"type\": \"Sigmoid\",\n",
    "                \"bottom\": \"dot_product\",\n",
    "                \"top\": \"sigmoid\"\n",
    "            },\n",
    "            \n",
    "            # Loss\n",
    "            {\n",
    "                \"name\": \"loss\",\n",
    "                \"type\": \"BinaryCrossEntropyLoss\",\n",
    "                \"bottom\": [\"sigmoid\", \"label\"],\n",
    "                \"top\": \"loss\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Create and save configuration\n",
    "config = create_hugectr_config()\n",
    "config_path = f\"{MODEL_DIR}/partner_recommender.json\"\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"HugeCTR configuration saved to: {config_path}\")\n",
    "print(\"Model architecture:\")\n",
    "print(f\"- Input features: {len(continuous_cols) + len(culture_cols)} dense + 2 sparse\")\n",
    "print(f\"- Tower architecture: 512 -> 256 -> 128\")\n",
    "print(f\"- Output: Dot product + Sigmoid\")\n",
    "print(f\"- Loss: Binary Cross Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train the HugeCTR two-tower model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train HugeCTR model\n",
    "print(\"Creating HugeCTR model...\")\n",
    "model = hugectr.Model(config, hugectr.Check_t.Sum)\n",
    "\n",
    "# Compile model\n",
    "print(\"Compiling model...\")\n",
    "model.compile()\n",
    "\n",
    "# Display model summary\n",
    "print(\"Model summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.fit(\n",
    "    max_iter=20000,\n",
    "    display=1000,\n",
    "    eval_interval=1000,\n",
    "    snapshot=10000\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(f\"Training completed in: {training_duration}\")\n",
    "print(f\"Model saved to: {MODEL_DIR}/partner_recommender_dense_*.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate the trained model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference\n",
    "print(\"Loading model for evaluation...\")\n",
    "\n",
    "inference_session = hugectr.inference.CreateInferenceSession(\n",
    "    model_config_path=config_path,\n",
    "    model_weights_path=f\"{MODEL_DIR}/partner_recommender_dense_20000.model\",\n",
    "    device_id=0,\n",
    "    cache_size_percentage=0.5,\n",
    "    i64_input_key=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully for inference!\")\n",
    "\n",
    "# Evaluate on validation set (sample)\n",
    "val_sample = val_gdf.sample(n=1000).to_pandas()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for idx, row in val_sample.iterrows():\n",
    "    try:\n",
    "        # Prepare features for inference\n",
    "        dense_features = np.array([\n",
    "            row[continuous_cols + culture_cols].values\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        sparse_features = np.array([\n",
    "            [row['company_a'], row['company_b']]\n",
    "        ], dtype=np.int64)\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = inference_session.predict(\n",
    "            dense_features,\n",
    "            [sparse_features[:, 0:1], sparse_features[:, 1:2]]\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred[0][0])\n",
    "        true_labels.append(row['label'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error for row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    'auc': roc_auc_score(true_labels, predictions),\n",
    "    'accuracy': accuracy_score(true_labels, binary_predictions),\n",
    "    'precision': precision_score(true_labels, binary_predictions),\n",
    "    'recall': recall_score(true_labels, binary_predictions),\n",
    "    'f1_score': f1_score(true_labels, binary_predictions)\n",
    "}\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = f\"{MODEL_DIR}/metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Create visualizations of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Prediction distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(predictions[true_labels == 0], bins=50, alpha=0.7, label='Negative', color='red')\n",
    "plt.hist(predictions[true_labels == 1], bins=50, alpha=0.7, label='Positive', color='blue')\n",
    "plt.xlabel('Prediction Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Distribution by True Label')\n",
    "plt.legend()\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(true_labels, binary_predictions)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{MODEL_DIR}/model_evaluation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Evaluation plots saved to: {MODEL_DIR}/model_evaluation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary\n",
    "\n",
    "Save training summary and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"model_info\": {\n",
    "        \"architecture\": \"Two-tower HugeCTR\",\n",
    "        \"framework\": \"NVIDIA HugeCTR\",\n",
    "        \"version\": \"23.08\",\n",
    "        \"embedding_dim\": 128\n",
    "    },\n",
    "    \"training_data\": {\n",
    "        \"total_samples\": len(df),\n",
    "        \"training_samples\": len(train_gdf),\n",
    "        \"validation_samples\": len(val_gdf),\n",
    "        \"positive_ratio\": float(df['label'].mean()),\n",
    "        \"features\": {\n",
    "            \"categorical\": len(categorical_cols),\n",
    "            \"continuous\": len(continuous_cols),\n",
    "            \"culture_vector\": len(culture_cols)\n",
    "        }\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"batch_size\": 1024,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"max_iterations\": 20000,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss_function\": \"Binary Cross Entropy\"\n",
    "    },\n",
    "    \"performance_metrics\": metrics,\n",
    "    \"training_time\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "        \"duration_seconds\": training_duration.total_seconds()\n",
    "    },\n",
    "    \"model_files\": {\n",
    "        \"config\": config_path,\n",
    "        \"weights\": f\"{MODEL_DIR}/partner_recommender_dense_20000.model\",\n",
    "        \"workflow\": workflow_path,\n",
    "        \"metrics\": metrics_path\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save training summary\n",
    "summary_path = f\"{MODEL_DIR}/training_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model files saved in: {MODEL_DIR}\")\n",
    "print(f\"Training summary: {summary_path}\")\n",
    "print(f\"Final AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"Training duration: {training_duration}\")\n",
    "print(\"\\nModel is ready for inference!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}